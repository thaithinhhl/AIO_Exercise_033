{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HOME\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['clean_text', 'category'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          clean_text  category\n",
      "0  when modi promised â€œminimum government maximum...      -1.0\n",
      "1  talk all the nonsense and continue all the dra...       0.0\n",
      "2  what did just say vote for modi  welcome bjp t...       1.0\n",
      "3  asking his supporters prefix chowkidar their n...       1.0\n",
      "4  answer who among these the most powerful world...       1.0\n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HOME\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load data\n",
    "file_path = \"C:\\\\Users\\\\HOME\\\\OneDrive\\\\Documents\\\\Downloads\\\\Twitter_Data.csv\"\n",
    "df = pd.read_csv(file_path)\n",
    "df = df.dropna()\n",
    "\n",
    "# Text normalization function\n",
    "def text_normalize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'^rt[\\s]+', '', text)\n",
    "    text = re.sub(r'https?://.*[\\r\\n]*', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [word for word in text.split() if word not in stop_words]\n",
    "    stemmer = SnowballStemmer('english')\n",
    "    words = [stemmer.stem(word) for word in words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# Apply text normalization to the 'clean_text' column\n",
    "df['clean_text'] = df['clean_text'].apply(text_normalize)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=2000)\n",
    "X = vectorizer.fit_transform(df['clean_text']).toarray()\n",
    "\n",
    "# Add bias to X\n",
    "intercept = np.ones((X.shape[0], 1))\n",
    "X_b = np.concatenate((intercept, X), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = df['category'].nunique()\n",
    "n_samples = df['category'].size\n",
    "y = df['category'].to_numpy()\n",
    "y_encoded = np.zeros((n_samples, n_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_size = 0.2\n",
    "test_size = 0.125\n",
    "random_state = 2\n",
    "is_shuffle = True\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_b, y_encoded, test_size=val_size, random_state=random_state, shuffle=is_shuffle\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train, y_train, test_size=test_size, random_state=random_state, shuffle=is_shuffle\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    exp_z = np.exp(z)\n",
    "    return exp_z / exp_z.sum(axis=1, keepdims=True)\n",
    "\n",
    "def predict(X, theta):\n",
    "    z = np.dot(X, theta)\n",
    "    return softmax(z)\n",
    "\n",
    "def compute_loss(y_hat, y):\n",
    "    n = y.size\n",
    "    return (-1 / n) * np.sum(y * np.log(y_hat))\n",
    "\n",
    "def compute_gradient(X, y, y_hat):\n",
    "    n = y.size\n",
    "    return np.dot(X.T, (y_hat - y)) / n\n",
    "\n",
    "def update_theta(theta, gradient, lr):\n",
    "    return theta - lr * gradient\n",
    "\n",
    "def compute_accuracy(X, y, theta):\n",
    "    y_hat = predict(X, theta)\n",
    "    return (np.argmax(y_hat, axis=1) == np.argmax(y, axis=1)).mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "epochs = 200\n",
    "batch_size = X_train.shape[0]\n",
    "n_features = X_train.shape[1]\n",
    "\n",
    "np.random.seed(random_state)\n",
    "theta = np.random.uniform(size=(n_features, n_classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Loss: -0.0000\n",
      "Epoch 10/200, Loss: -0.0000\n",
      "Epoch 20/200, Loss: -0.0000\n",
      "Epoch 30/200, Loss: -0.0000\n",
      "Epoch 40/200, Loss: -0.0000\n",
      "Epoch 50/200, Loss: -0.0000\n",
      "Epoch 60/200, Loss: -0.0000\n",
      "Epoch 70/200, Loss: -0.0000\n",
      "Epoch 80/200, Loss: -0.0000\n",
      "Epoch 90/200, Loss: -0.0000\n",
      "Epoch 100/200, Loss: -0.0000\n",
      "Epoch 110/200, Loss: -0.0000\n",
      "Epoch 120/200, Loss: -0.0000\n",
      "Epoch 130/200, Loss: -0.0000\n",
      "Epoch 140/200, Loss: -0.0000\n",
      "Epoch 150/200, Loss: -0.0000\n",
      "Epoch 160/200, Loss: -0.0000\n",
      "Epoch 170/200, Loss: -0.0000\n",
      "Epoch 180/200, Loss: -0.0000\n",
      "Epoch 190/200, Loss: -0.0000\n",
      "Epoch 200/200, Loss: -0.0000\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "for epoch in range(epochs):\n",
    "    # Forward pass: Compute predictions\n",
    "    y_hat = predict(X_train, theta)\n",
    "    \n",
    "    # Calculate loss\n",
    "    train_loss = compute_loss(y_hat, y_train)\n",
    "    \n",
    "    # Compute gradient\n",
    "    gradient = compute_gradient(X_train, y_train, y_hat)\n",
    "    \n",
    "    # Update weights\n",
    "    theta = update_theta(theta, gradient, lr)\n",
    "    \n",
    "    # Optional: Print the training loss every few epochs\n",
    "    if (epoch + 1) % 10 == 0 or epoch == 0:\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.3551\n",
      "Test Accuracy: 0.3543\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model on validation and test sets\n",
    "val_set_acc = compute_accuracy(X_val, y_val, theta)\n",
    "test_set_acc = compute_accuracy(X_test, y_test, theta)\n",
    "print(f'Validation Accuracy: {val_set_acc:.4f}')\n",
    "print(f'Test Accuracy: {test_set_acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
